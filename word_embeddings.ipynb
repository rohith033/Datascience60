{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re \nimport torch \nimport torch.nn as nn\nimport sklearn\nimport torch.nn.functional as F\nfrom nltk.tokenize import word_tokenize\nfrom torch.utils.data import DataLoader,Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T18:33:00.105262Z","iopub.execute_input":"2023-07-07T18:33:00.105681Z","iopub.status.idle":"2023-07-07T18:33:04.840641Z","shell.execute_reply.started":"2023-07-07T18:33:00.105633Z","shell.execute_reply":"2023-07-07T18:33:04.839561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:04.842799Z","iopub.execute_input":"2023-07-07T18:33:04.843404Z","iopub.status.idle":"2023-07-07T18:33:04.915215Z","shell.execute_reply.started":"2023-07-07T18:33:04.843371Z","shell.execute_reply":"2023-07-07T18:33:04.914210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **document similarty score**\n*this one of the fundemental area of NLP there are several methods proposed to do this taks*\n\n**top methods**\n\n1. Cosine Similarity\n*here we will be converting documents into vectors and apply for this vector conversion we normally use Term Frequency-Inverse Document Frequency and the dot product between two docs represent the similarity*\n\n*this method doesnt take capture the semantic meaning of words for example teacher and professor are consider like different individual*\n\n2. Word Embedding-Based Methods\n*in this method we fine-tune/train numerical word embeddings based on their occurance in a sentence*\n*for this there are two widely used architecture are 'Continuous Bag-of-Words' and 'skip gram'*\n\n**Continuous Bag-of-Words**\n*predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word.*\n\n**example**\n> i am rohith currenty pursuing btech \n\n> given \"i am rohith ---- pursuing btech\" the model will predict the currently\n\n> we will be updating the embedding layer weights through backprop\n\n**skip gram model**\n*predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.*\n\n**example**\n\n> i am rohith currenty pursuing btech \n\n> given \"pursuing\" the model will predict the i am rohith  pursuing btech\n\n> we will be updating embedding layer weights through backprop\n\n**skip gram model is able to get better semantic and Syntactic accuracy then cbow model so we will be implementing the skip gram model to get the word embeddings**\n\n*source https://arxiv.org/pdf/1301.3781.pdf (Efficient Estimation of Word Representations in Vector Space)*","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/document-classification/file.txt','r') as f:\n    data = f.read()\ndata = segments = re.split(r'\\n[1-9]', data)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:04.916911Z","iopub.execute_input":"2023-07-07T18:33:04.917534Z","iopub.status.idle":"2023-07-07T18:33:04.968337Z","shell.execute_reply.started":"2023-07-07T18:33:04.917500Z","shell.execute_reply":"2023-07-07T18:33:04.967482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_corpus = []\nfor sentence in data:\n    tokens = word_tokenize(sentence)\n    tokenized_corpus.extend(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:04.970942Z","iopub.execute_input":"2023-07-07T18:33:04.971508Z","iopub.status.idle":"2023-07-07T18:33:10.262118Z","shell.execute_reply.started":"2023-07-07T18:33:04.971459Z","shell.execute_reply":"2023-07-07T18:33:10.261163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_corpus[1000])","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:10.263517Z","iopub.execute_input":"2023-07-07T18:33:10.264275Z","iopub.status.idle":"2023-07-07T18:33:10.271731Z","shell.execute_reply.started":"2023-07-07T18:33:10.264240Z","shell.execute_reply":"2023-07-07T18:33:10.270557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 1\nword_to_num_dict = {}\nfor word in tokenized_corpus:\n    if(word not in word_to_num_dict.keys()):\n        word_to_num_dict[word]=idx\n        idx+=1\nvocab_size = len(word_to_num_dict)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:10.273349Z","iopub.execute_input":"2023-07-07T18:33:10.273941Z","iopub.status.idle":"2023-07-07T18:33:10.430280Z","shell.execute_reply.started":"2023-07-07T18:33:10.273908Z","shell.execute_reply":"2023-07-07T18:33:10.429405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_to_word_dict = {value: key for key, value in word_to_num_dict.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:10.431696Z","iopub.execute_input":"2023-07-07T18:33:10.432044Z","iopub.status.idle":"2023-07-07T18:33:10.441515Z","shell.execute_reply.started":"2023-07-07T18:33:10.432012Z","shell.execute_reply":"2023-07-07T18:33:10.440517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**the dataset contains 19982 unique words** ","metadata":{}},{"cell_type":"code","source":"x = []\ny = []\nfor token in tokenized_corpus:\n    for i in range(2, len(token) - 2):\n        center_word_idx = word_to_num_dict[token[i]]\n        context_word_indices = [word_to_num_dict[token[j]] for j in range(i-2, i+3) if j != i] \n        x.append(center_word_idx)\n        y.append(context_word_indices)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:10.443426Z","iopub.execute_input":"2023-07-07T18:33:10.444067Z","iopub.status.idle":"2023-07-07T18:33:13.537874Z","shell.execute_reply.started":"2023-07-07T18:33:10.444034Z","shell.execute_reply":"2023-07-07T18:33:13.536674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(x)\ny = torch.tensor(y)\nx.to(device)\ny.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:13.539512Z","iopub.execute_input":"2023-07-07T18:33:13.539906Z","iopub.status.idle":"2023-07-07T18:33:18.978169Z","shell.execute_reply.started":"2023-07-07T18:33:13.539854Z","shell.execute_reply":"2023-07-07T18:33:18.977258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset and DataLoader**","metadata":{}},{"cell_type":"code","source":"class customdataset(Dataset):\n    def __init__(self,x,y):\n        self.inputs = x\n        self.outputs = y\n    def __len__(self):\n        return len(self.inputs)\n    def __getitem__(self,idx):\n        return self.inputs[idx],self.outputs[idx]","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:18.984403Z","iopub.execute_input":"2023-07-07T18:33:18.987000Z","iopub.status.idle":"2023-07-07T18:33:18.995536Z","shell.execute_reply.started":"2023-07-07T18:33:18.986964Z","shell.execute_reply":"2023-07-07T18:33:18.994157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = customdataset(x,y)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:19.001417Z","iopub.execute_input":"2023-07-07T18:33:19.004921Z","iopub.status.idle":"2023-07-07T18:33:19.012113Z","shell.execute_reply.started":"2023-07-07T18:33:19.004877Z","shell.execute_reply":"2023-07-07T18:33:19.010790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Loader = DataLoader(dataset,batch_size=128,num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:19.018283Z","iopub.execute_input":"2023-07-07T18:33:19.021254Z","iopub.status.idle":"2023-07-07T18:33:19.028871Z","shell.execute_reply.started":"2023-07-07T18:33:19.021212Z","shell.execute_reply":"2023-07-07T18:33:19.027661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Skip gram model**","metadata":{}},{"cell_type":"code","source":"class skip_gram(nn.Module):\n    def __init__(self,vocab_size,dimension):\n        super(skip_gram,self).__init__()\n        self.dimension=dimension\n        self.target_embedding = nn.Embedding(vocab_size,dimension)\n        self.context_embedding = nn.Embedding(vocab_size,dimension)\n    def forward(self,target,context):\n        if(len(target.size())==2):\n            target=target.squeeze(axis=1)\n        target_embedding = self.target_embedding(target)\n#         print(target_embedding.shape)\n        context_embedding = self.context_embedding(context)\n#         print(context_embedding.shape)\n        return torch.einsum('be,bce->bc',target_embedding,context_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:19.033255Z","iopub.execute_input":"2023-07-07T18:33:19.034029Z","iopub.status.idle":"2023-07-07T18:33:19.047688Z","shell.execute_reply.started":"2023-07-07T18:33:19.033990Z","shell.execute_reply":"2023-07-07T18:33:19.046399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = skip_gram(19983,300).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:19.053434Z","iopub.execute_input":"2023-07-07T18:33:19.054163Z","iopub.status.idle":"2023-07-07T18:33:19.318412Z","shell.execute_reply.started":"2023-07-07T18:33:19.054128Z","shell.execute_reply":"2023-07-07T18:33:19.317410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim \noptimizer = optim.Adam(model.parameters(),lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:33:19.319870Z","iopub.execute_input":"2023-07-07T18:33:19.320431Z","iopub.status.idle":"2023-07-07T18:33:19.329781Z","shell.execute_reply.started":"2023-07-07T18:33:19.320395Z","shell.execute_reply":"2023-07-07T18:33:19.328698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nepochs = 200\nlosses = []\nbest_run = 100\nfor e in range(epochs):\n    net_batch_loss = 0\n    cnt = 0\n    early_stop = 0\n    for target,context in tqdm(Loader):\n        batch_size = target.shape[0]\n        optimizer.zero_grad()\n        pred = model(target.to(device),context.to(device))\n        ground_truth = torch.ones(batch_size*4).view(batch_size,4)\n        ground_truth = ground_truth.to(device)\n        loss = F.cross_entropy(pred,ground_truth)\n        loss.backward()\n        optimizer.step()\n        cnt+=1\n        net_batch_loss+=loss.item()\n    if(net_batch_loss/cnt<best_run):\n        torch.save(model.state_dict(),f'/kaggle/working/{e}th.pth')\n        best_run = net_batch_loss/cnt\n        early_stop = 0\n    early_stop+=1\n    losses.append(net_batch_loss/cnt)\n    print(f'loss at the {e}th epoch is {net_batch_loss/cnt}')\n    if(early_stop>=5):\n        print('early stopping due to no improvement')\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_trained = torch.load('/kaggle/working/4th.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:57:48.037619Z","iopub.execute_input":"2023-07-07T18:57:48.038407Z","iopub.status.idle":"2023-07-07T18:57:48.067394Z","shell.execute_reply.started":"2023-07-07T18:57:48.038371Z","shell.execute_reply":"2023-07-07T18:57:48.066463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_trained.keys()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:58:49.302260Z","iopub.execute_input":"2023-07-07T18:58:49.302668Z","iopub.status.idle":"2023-07-07T18:58:49.308908Z","shell.execute_reply.started":"2023-07-07T18:58:49.302634Z","shell.execute_reply":"2023-07-07T18:58:49.307948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = pre_trained['target_embedding.weight']","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:59:08.262446Z","iopub.execute_input":"2023-07-07T18:59:08.262843Z","iopub.status.idle":"2023-07-07T18:59:08.267363Z","shell.execute_reply.started":"2023-07-07T18:59:08.262811Z","shell.execute_reply":"2023-07-07T18:59:08.266380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = weights.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T19:05:18.412324Z","iopub.execute_input":"2023-07-07T19:05:18.412682Z","iopub.status.idle":"2023-07-07T19:05:18.422841Z","shell.execute_reply.started":"2023-07-07T19:05:18.412653Z","shell.execute_reply":"2023-07-07T19:05:18.421787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nreduced_data = pca.fit_transform(weights)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T19:06:14.842199Z","iopub.execute_input":"2023-07-07T19:06:14.842586Z","iopub.status.idle":"2023-07-07T19:06:15.210755Z","shell.execute_reply.started":"2023-07-07T19:06:14.842553Z","shell.execute_reply":"2023-07-07T19:06:15.209354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_values = [point[0] for point in reduced_data[300:500]]\ny_values = [point[1] for point in reduced_data[300:500]]\ntitles = [ num_to_word_dict[point] for point in range(300,500)]\nplt.rcParams.update({'font.size': 12})\nplt.figure(figsize=(20,20)) \nplt.scatter(x_values, y_values)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter Plot')\nfor i, title in enumerate(titles):\n    plt.text(x_values[i], y_values[i], title)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-07T19:23:06.857564Z","iopub.execute_input":"2023-07-07T19:23:06.857934Z","iopub.status.idle":"2023-07-07T19:23:07.841811Z","shell.execute_reply.started":"2023-07-07T19:23:06.857905Z","shell.execute_reply":"2023-07-07T19:23:07.837913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1 = weights[word_to_num_dict['office']]\np2 = weights[word_to_num_dict['boss']]\nx_val = [p1[0],p2[0]]\ny_val = [p1[1],p2[1]]\nplt.scatter(x_val,y_val)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T19:28:14.412296Z","iopub.execute_input":"2023-07-07T19:28:14.412674Z","iopub.status.idle":"2023-07-07T19:28:14.692364Z","shell.execute_reply.started":"2023-07-07T19:28:14.412644Z","shell.execute_reply":"2023-07-07T19:28:14.691393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}