{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re \nimport torch \nimport torch.nn as nn\nimport sklearn\nimport torch.nn.functional as F\nfrom nltk.tokenize import word_tokenize\nfrom torch.utils.data import DataLoader,Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T12:36:08.419244Z","iopub.execute_input":"2023-07-07T12:36:08.419689Z","iopub.status.idle":"2023-07-07T12:36:08.426182Z","shell.execute_reply.started":"2023-07-07T12:36:08.419650Z","shell.execute_reply":"2023-07-07T12:36:08.424668Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available:\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:28:25.411150Z","iopub.execute_input":"2023-07-07T14:28:25.411611Z","iopub.status.idle":"2023-07-07T14:28:25.421289Z","shell.execute_reply.started":"2023-07-07T14:28:25.411580Z","shell.execute_reply":"2023-07-07T14:28:25.419446Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **document similarty score**\n*this one of the fundemental area of NLP there are several methods proposed to do this taks*\n\n**top methods**\n\n1. Cosine Similarity\n*here we will be converting documents into vectors and apply for this vector conversion we normally use Term Frequency-Inverse Document Frequency and the dot product between two docs represent the similarity*\n\n*this method doesnt take capture the semantic meaning of words for example teacher and professor are consider like different individual*\n\n2. Word Embedding-Based Methods\n*in this method we fine-tune/train numerical word embeddings based on their occurance in a sentence*\n*for this there are two widely used architecture are 'Continuous Bag-of-Words' and 'skip gram'*\n\n**Continuous Bag-of-Words**\n*predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word.*\n\n**example**\n> i am rohith currenty pursuing btech \n\n> given \"i am rohith ---- pursuing btech\" the model will predict the currently\n\n> we will be updating the embedding layer weights through backprop\n\n**skip gram model**\n*predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.*\n\n**example**\n\n> i am rohith currenty pursuing btech \n\n> given \"pursuing\" the model will predict the i am rohith  pursuing btech\n\n> we will be updating embedding layer weights through backprop\n\n**skip gram model is able to get better semantic and Syntactic accuracy then cbow model so we will be implementing the skip gram model to get the word embeddings**\n\n*source https://arxiv.org/pdf/1301.3781.pdf (Efficient Estimation of Word Representations in Vector Space)*","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/document-classification/file.txt','r') as f:\n    data = f.read()\ndata = segments = re.split(r'\\n[1-9]', data)\ndata[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:36:16.948158Z","iopub.execute_input":"2023-07-07T12:36:16.948522Z","iopub.status.idle":"2023-07-07T12:36:16.978603Z","shell.execute_reply.started":"2023-07-07T12:36:16.948495Z","shell.execute_reply":"2023-07-07T12:36:16.977077Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"' champion products ch approves stock split champion products inc said its board of directors approved a two for one stock split of its common shares for shareholders of record as of april the company also said its board voted to recommend to shareholders at the annual meeting april an increase in the authorized capital stock from five mln to mln shares reuter '"},"metadata":{}}]},{"cell_type":"code","source":"idx = 1\nword_to_num_dict = {}\nfor token in tokenized_corpus:\n    for word in token:\n        if(word not in word_to_num_dict.keys()):\n            word_to_num_dict[word]=idx\n            idx+=1\nvocab_size = len(word_to_num_dict)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:36:22.997658Z","iopub.execute_input":"2023-07-07T12:36:22.998064Z","iopub.status.idle":"2023-07-07T12:36:23.166811Z","shell.execute_reply.started":"2023-07-07T12:36:22.998031Z","shell.execute_reply":"2023-07-07T12:36:23.165667Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"19982"},"metadata":{}}]},{"cell_type":"code","source":"num_to_word_dict = {value: key for key, value in word_to_num_dict.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:16:33.536256Z","iopub.execute_input":"2023-07-07T14:16:33.536627Z","iopub.status.idle":"2023-07-07T14:16:33.548644Z","shell.execute_reply.started":"2023-07-07T14:16:33.536600Z","shell.execute_reply":"2023-07-07T14:16:33.547011Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"**the dataset contains 19982 unique words** ","metadata":{}},{"cell_type":"code","source":"x = []\ny = []\nfor token in tokenized_corpus:\n    for i in range(2, len(token) - 2):\n        center_word_idx = word_to_num_dict[token[i]]\n        context_word_indices = [word_to_num_dict[token[j]] for j in range(i-2, i+3) if j != i] \n        x.append(center_word_idx)\n        y.append(context_word_indices)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:09:27.715081Z","iopub.execute_input":"2023-07-07T14:09:27.715555Z","iopub.status.idle":"2023-07-07T14:09:29.755742Z","shell.execute_reply.started":"2023-07-07T14:09:27.715522Z","shell.execute_reply":"2023-07-07T14:09:29.754348Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(x)\ny = torch.tensor(y)\nx.shape , y.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:29:16.190461Z","iopub.execute_input":"2023-07-07T14:29:16.191789Z","iopub.status.idle":"2023-07-07T14:29:16.212778Z","shell.execute_reply.started":"2023-07-07T14:29:16.191746Z","shell.execute_reply":"2023-07-07T14:29:16.211408Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2346925435.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x = torch.tensor(x)\n/tmp/ipykernel_32/2346925435.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(y)\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(torch.Size([555557]), torch.Size([555557, 4]))"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dataset and DataLoader**","metadata":{}},{"cell_type":"code","source":"class customdataset(Dataset):\n    def __init__(self,x,y):\n        self.inputs = x\n        self.outputs = y\n    def __len__(self):\n        return len(self.input)\n    def __getitem__(self,idx):\n        return self.input[idx].to(device),self.output[idx].to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:29:31.115305Z","iopub.execute_input":"2023-07-07T14:29:31.115766Z","iopub.status.idle":"2023-07-07T14:29:31.123232Z","shell.execute_reply.started":"2023-07-07T14:29:31.115732Z","shell.execute_reply":"2023-07-07T14:29:31.121871Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"dataset = customdataset(x,y)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:29:35.025320Z","iopub.execute_input":"2023-07-07T14:29:35.025791Z","iopub.status.idle":"2023-07-07T14:29:35.032376Z","shell.execute_reply.started":"2023-07-07T14:29:35.025757Z","shell.execute_reply":"2023-07-07T14:29:35.030859Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"Loader = DataLoader(dataset,batch_size=128,num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:29:37.905315Z","iopub.execute_input":"2023-07-07T14:29:37.905699Z","iopub.status.idle":"2023-07-07T14:29:37.911827Z","shell.execute_reply.started":"2023-07-07T14:29:37.905670Z","shell.execute_reply":"2023-07-07T14:29:37.910533Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"**Skip gram model**","metadata":{}},{"cell_type":"code","source":"class skip_gram(nn.Module):\n    def __init__(self,vocab_size,dimension):\n        super(skip_gram,self).__init__()\n        self.dimension=dimension\n        self.target_embedding = nn.Embedding(vocab_size,dimension)\n        self.context_embedding = nn.Embedding(vocab_size,dimension)\n    def forward(self,target,context):\n        if(len(target.size())==2):\n            target=target.squeeze(axis=1)\n        target_embedding = self.target_embedding(target)\n        print(target_embedding.shape)\n        context_embedding = self.context_embedding(context)\n        print(context_embedding.shape)\n        return torch.einsum('be,bce->bc', target_embedding,context_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:49:13.613576Z","iopub.execute_input":"2023-07-07T14:49:13.613987Z","iopub.status.idle":"2023-07-07T14:49:13.622356Z","shell.execute_reply.started":"2023-07-07T14:49:13.613955Z","shell.execute_reply":"2023-07-07T14:49:13.620991Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"model = skip_gram(19983,5)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:49:16.512437Z","iopub.execute_input":"2023-07-07T14:49:16.512876Z","iopub.status.idle":"2023-07-07T14:49:16.520950Z","shell.execute_reply.started":"2023-07-07T14:49:16.512847Z","shell.execute_reply":"2023-07-07T14:49:16.519506Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim \noptimizer = optim.Adam(model.parameters(),lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:49:17.802987Z","iopub.execute_input":"2023-07-07T14:49:17.803421Z","iopub.status.idle":"2023-07-07T14:49:17.810036Z","shell.execute_reply.started":"2023-07-07T14:49:17.803390Z","shell.execute_reply":"2023-07-07T14:49:17.808057Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:49:20.752732Z","iopub.execute_input":"2023-07-07T14:49:20.753209Z","iopub.status.idle":"2023-07-07T14:49:20.761380Z","shell.execute_reply.started":"2023-07-07T14:49:20.753178Z","shell.execute_reply":"2023-07-07T14:49:20.759779Z"},"trusted":true},"execution_count":149,"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"skip_gram(\n  (target_embedding): Embedding(19983, 5)\n  (context_embedding): Embedding(19983, 5)\n)"},"metadata":{}}]},{"cell_type":"code","source":"test = model(torch.tensor([1,2]).view(2,1),torch.tensor([[1000,3,4,5],[3,6,7,8]]).view(2,4))\ntest","metadata":{"execution":{"iopub.status.busy":"2023-07-07T14:50:11.383396Z","iopub.execute_input":"2023-07-07T14:50:11.383860Z","iopub.status.idle":"2023-07-07T14:50:11.397925Z","shell.execute_reply.started":"2023-07-07T14:50:11.383827Z","shell.execute_reply":"2023-07-07T14:50:11.396270Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"torch.Size([2, 5])\ntorch.Size([2, 4, 5])\n","output_type":"stream"},{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1.3086,  3.3790, -0.4160,  3.6633],\n        [ 2.8049,  0.1689, -0.5356,  2.8980]], grad_fn=<ViewBackward0>)"},"metadata":{}}]}]}