{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport re \nimport torch \nimport torch.nn as nn\nimport sklearn\nimport torch.nn.functional as F\nfrom nltk.tokenize import word_tokenize\nfrom torch.utils.data import DataLoader,Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T18:12:54.887010Z","iopub.execute_input":"2023-07-07T18:12:54.887668Z","iopub.status.idle":"2023-07-07T18:12:54.893622Z","shell.execute_reply.started":"2023-07-07T18:12:54.887637Z","shell.execute_reply":"2023-07-07T18:12:54.892709Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:12:54.895197Z","iopub.execute_input":"2023-07-07T18:12:54.896118Z","iopub.status.idle":"2023-07-07T18:12:54.907977Z","shell.execute_reply.started":"2023-07-07T18:12:54.896086Z","shell.execute_reply":"2023-07-07T18:12:54.907154Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **document similarty score**\n*this one of the fundemental area of NLP there are several methods proposed to do this taks*\n\n**top methods**\n\n1. Cosine Similarity\n*here we will be converting documents into vectors and apply for this vector conversion we normally use Term Frequency-Inverse Document Frequency and the dot product between two docs represent the similarity*\n\n*this method doesnt take capture the semantic meaning of words for example teacher and professor are consider like different individual*\n\n2. Word Embedding-Based Methods\n*in this method we fine-tune/train numerical word embeddings based on their occurance in a sentence*\n*for this there are two widely used architecture are 'Continuous Bag-of-Words' and 'skip gram'*\n\n**Continuous Bag-of-Words**\n*predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word.*\n\n**example**\n> i am rohith currenty pursuing btech \n\n> given \"i am rohith ---- pursuing btech\" the model will predict the currently\n\n> we will be updating the embedding layer weights through backprop\n\n**skip gram model**\n*predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.*\n\n**example**\n\n> i am rohith currenty pursuing btech \n\n> given \"pursuing\" the model will predict the i am rohith  pursuing btech\n\n> we will be updating embedding layer weights through backprop\n\n**skip gram model is able to get better semantic and Syntactic accuracy then cbow model so we will be implementing the skip gram model to get the word embeddings**\n\n*source https://arxiv.org/pdf/1301.3781.pdf (Efficient Estimation of Word Representations in Vector Space)*","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/document-classification/file.txt','r') as f:\n    data = f.read()\ndata = segments = re.split(r'\\n[1-9]', data)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:12:54.911050Z","iopub.execute_input":"2023-07-07T18:12:54.911371Z","iopub.status.idle":"2023-07-07T18:12:54.958301Z","shell.execute_reply.started":"2023-07-07T18:12:54.911348Z","shell.execute_reply":"2023-07-07T18:12:54.957453Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenized_corpus = []\nfor sentence in data:\n    tokens = word_tokenize(sentence)\n    tokenized_corpus.extend(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:12:54.959506Z","iopub.execute_input":"2023-07-07T18:12:54.960342Z","iopub.status.idle":"2023-07-07T18:13:00.038844Z","shell.execute_reply.started":"2023-07-07T18:12:54.960312Z","shell.execute_reply":"2023-07-07T18:13:00.037898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(tokenized_corpus[1000])","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:13:00.041462Z","iopub.execute_input":"2023-07-07T18:13:00.041911Z","iopub.status.idle":"2023-07-07T18:13:00.048945Z","shell.execute_reply.started":"2023-07-07T18:13:00.041877Z","shell.execute_reply":"2023-07-07T18:13:00.047878Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"div\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 1\nword_to_num_dict = {}\nfor word in tokenized_corpus:\n    if(word not in word_to_num_dict.keys()):\n        word_to_num_dict[word]=idx\n        idx+=1\nvocab_size = len(word_to_num_dict)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:13:00.050486Z","iopub.execute_input":"2023-07-07T18:13:00.051046Z","iopub.status.idle":"2023-07-07T18:13:00.198024Z","shell.execute_reply.started":"2023-07-07T18:13:00.051015Z","shell.execute_reply":"2023-07-07T18:13:00.197087Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"19982"},"metadata":{}}]},{"cell_type":"code","source":"num_to_word_dict = {value: key for key, value in word_to_num_dict.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:13:00.199333Z","iopub.execute_input":"2023-07-07T18:13:00.199766Z","iopub.status.idle":"2023-07-07T18:13:00.207639Z","shell.execute_reply.started":"2023-07-07T18:13:00.199731Z","shell.execute_reply":"2023-07-07T18:13:00.206651Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**the dataset contains 19982 unique words** ","metadata":{}},{"cell_type":"code","source":"x = []\ny = []\nfor token in tokenized_corpus:\n    for i in range(2, len(token) - 2):\n        center_word_idx = word_to_num_dict[token[i]]\n        context_word_indices = [word_to_num_dict[token[j]] for j in range(i-2, i+3) if j != i] \n        x.append(center_word_idx)\n        y.append(context_word_indices)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:13:00.209268Z","iopub.execute_input":"2023-07-07T18:13:00.209998Z","iopub.status.idle":"2023-07-07T18:13:03.310180Z","shell.execute_reply.started":"2023-07-07T18:13:00.209943Z","shell.execute_reply":"2023-07-07T18:13:03.309206Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(x)\ny = torch.tensor(y)\nx.to(device)\ny.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:13:03.311383Z","iopub.execute_input":"2023-07-07T18:13:03.311939Z","iopub.status.idle":"2023-07-07T18:13:08.623031Z","shell.execute_reply.started":"2023-07-07T18:13:03.311907Z","shell.execute_reply":"2023-07-07T18:13:08.622036Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[1127, 2061, 2160,  480],\n        [2061,   15,  480,  271],\n        [  15, 2160,  271,  377],\n        ...,\n        [  15,  458,  377,   57],\n        [1126,  269,  458,  269],\n        [ 269,  489,  269, 1126]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dataset and DataLoader**","metadata":{}},{"cell_type":"code","source":"class customdataset(Dataset):\n    def __init__(self,x,y):\n        self.inputs = x\n        self.outputs = y\n    def __len__(self):\n        return len(self.inputs)\n    def __getitem__(self,idx):\n        return self.inputs[idx],self.outputs[idx]","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:35.857788Z","iopub.execute_input":"2023-07-07T18:14:35.858195Z","iopub.status.idle":"2023-07-07T18:14:35.865402Z","shell.execute_reply.started":"2023-07-07T18:14:35.858156Z","shell.execute_reply":"2023-07-07T18:14:35.864080Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"dataset = customdataset(x,y)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:38.536989Z","iopub.execute_input":"2023-07-07T18:14:38.537932Z","iopub.status.idle":"2023-07-07T18:14:38.543057Z","shell.execute_reply.started":"2023-07-07T18:14:38.537896Z","shell.execute_reply":"2023-07-07T18:14:38.541659Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"Loader = DataLoader(dataset,batch_size=128,num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:40.986877Z","iopub.execute_input":"2023-07-07T18:14:40.987265Z","iopub.status.idle":"2023-07-07T18:14:40.991986Z","shell.execute_reply.started":"2023-07-07T18:14:40.987234Z","shell.execute_reply":"2023-07-07T18:14:40.990907Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Skip gram model**","metadata":{}},{"cell_type":"code","source":"class skip_gram(nn.Module):\n    def __init__(self,vocab_size,dimension):\n        super(skip_gram,self).__init__()\n        self.dimension=dimension\n        self.target_embedding = nn.Embedding(vocab_size,dimension)\n        self.context_embedding = nn.Embedding(vocab_size,dimension)\n    def forward(self,target,context):\n        if(len(target.size())==2):\n            target=target.squeeze(axis=1)\n        target_embedding = self.target_embedding(target)\n#         print(target_embedding.shape)\n        context_embedding = self.context_embedding(context)\n#         print(context_embedding.shape)\n        return torch.einsum('be,bce->bc',target_embedding,context_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:44.257027Z","iopub.execute_input":"2023-07-07T18:14:44.258147Z","iopub.status.idle":"2023-07-07T18:14:44.265606Z","shell.execute_reply.started":"2023-07-07T18:14:44.258091Z","shell.execute_reply":"2023-07-07T18:14:44.264486Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = skip_gram(19983,5).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:47.582114Z","iopub.execute_input":"2023-07-07T18:14:47.583246Z","iopub.status.idle":"2023-07-07T18:14:47.590896Z","shell.execute_reply.started":"2023-07-07T18:14:47.583207Z","shell.execute_reply":"2023-07-07T18:14:47.589923Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim \noptimizer = optim.Adam(model.parameters(),lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T18:14:50.631911Z","iopub.execute_input":"2023-07-07T18:14:50.632292Z","iopub.status.idle":"2023-07-07T18:14:50.636996Z","shell.execute_reply.started":"2023-07-07T18:14:50.632261Z","shell.execute_reply":"2023-07-07T18:14:50.636052Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.train()\nepochs = 200\nlosses = []\nfor e in range(epochs):\n    net_batch_loss = 0\n    cnt = 0\n    for target,context in tqdm(Loader):\n        batch_size = target.shape[0]\n        optimizer.zero_grad()\n        pred = model(target.to(device),context.to(device))\n        ground_truth = torch.ones(batch_size*4).view(batch_size,4)\n        ground_truth = ground_truth.to(device)\n        loss = F.cross_entropy(pred,ground_truth)\n        loss.backward()\n        optimizer.step()\n        cnt+=1\n        net_batch_loss+=loss.item()\n    losses.append(net_batch_loss/cnt)\n    print(f'loss at the {e}th epoch is {net_batch_loss/cnt}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}