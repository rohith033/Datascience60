{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport torch.optim as optim\n!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:53:58.093979Z","iopub.execute_input":"2023-07-03T12:53:58.094682Z","iopub.status.idle":"2023-07-03T12:54:09.531623Z","shell.execute_reply.started":"2023-07-03T12:53:58.094649Z","shell.execute_reply":"2023-07-03T12:54:09.530353Z"},"trusted":true},"execution_count":444,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.535225Z","iopub.execute_input":"2023-07-03T12:54:09.535624Z","iopub.status.idle":"2023-07-03T12:54:09.542793Z","shell.execute_reply.started":"2023-07-03T12:54:09.535589Z","shell.execute_reply":"2023-07-03T12:54:09.541650Z"},"trusted":true},"execution_count":445,"outputs":[{"execution_count":445,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"with open('/kaggle/input/popular-names/names.txt','r') as file:\n    data = file.read().split('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.544332Z","iopub.execute_input":"2023-07-03T12:54:09.544684Z","iopub.status.idle":"2023-07-03T12:54:09.562483Z","shell.execute_reply.started":"2023-07-03T12:54:09.544652Z","shell.execute_reply":"2023-07-03T12:54:09.561200Z"},"trusted":true},"execution_count":446,"outputs":[]},{"cell_type":"code","source":"data[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.566856Z","iopub.execute_input":"2023-07-03T12:54:09.567166Z","iopub.status.idle":"2023-07-03T12:54:09.573526Z","shell.execute_reply.started":"2023-07-03T12:54:09.567139Z","shell.execute_reply":"2023-07-03T12:54:09.572410Z"},"trusted":true},"execution_count":447,"outputs":[{"execution_count":447,"output_type":"execute_result","data":{"text/plain":"['emma', 'olivia', 'ava', 'isabella', 'sophia']"},"metadata":{}}]},{"cell_type":"code","source":"train_data = data[:int(len(data)*0.8)]\nval_data = data[int(len(data)*0.8):int(len(data)*0.9)]\ntest_data = data[int(len(data)*0.9):int(len(data))]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.574729Z","iopub.execute_input":"2023-07-03T12:54:09.575291Z","iopub.status.idle":"2023-07-03T12:54:09.583739Z","shell.execute_reply.started":"2023-07-03T12:54:09.575255Z","shell.execute_reply":"2023-07-03T12:54:09.582760Z"},"trusted":true},"execution_count":448,"outputs":[]},{"cell_type":"code","source":"print(len(train_data) , len(val_data) , len(test_data))","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.585176Z","iopub.execute_input":"2023-07-03T12:54:09.586126Z","iopub.status.idle":"2023-07-03T12:54:09.596611Z","shell.execute_reply.started":"2023-07-03T12:54:09.586093Z","shell.execute_reply":"2023-07-03T12:54:09.595357Z"},"trusted":true},"execution_count":449,"outputs":[{"name":"stdout","text":"25626 3203 3204\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**dataset**\n*the below function create a dataset given data and chuck size (here a chunck mean how  many char given to predict the next char)*","metadata":{}},{"cell_type":"code","source":"def makedataset(data,chunck):\n    x = []\n    y = []\n    dummy=''\n    for i in range(chunck):\n        dummy+='.'\n    for word in data:\n        word = dummy + word + '.'\n        for i in range(len(word)-chunck-1):\n            x.append(word[i:i+chunck])\n            y.append(word[i+chunck])\n    return x , y","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.598196Z","iopub.execute_input":"2023-07-03T12:54:09.598762Z","iopub.status.idle":"2023-07-03T12:54:09.612267Z","shell.execute_reply.started":"2023-07-03T12:54:09.598730Z","shell.execute_reply":"2023-07-03T12:54:09.611251Z"},"trusted":true},"execution_count":450,"outputs":[]},{"cell_type":"code","source":"x , y = makedataset(train_data,4)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.615669Z","iopub.execute_input":"2023-07-03T12:54:09.615963Z","iopub.status.idle":"2023-07-03T12:54:09.706592Z","shell.execute_reply.started":"2023-07-03T12:54:09.615937Z","shell.execute_reply":"2023-07-03T12:54:09.705622Z"},"trusted":true},"execution_count":451,"outputs":[]},{"cell_type":"markdown","source":"**use some type maping to make them into number so that we can do operations on them**\n\n*one way is one hot encodding where echar will be represented by a list of size with vocublary length*\n*othre way is we can represent each char into a lower dimensional vector which helps in increasing computaional speed and data requriments*","metadata":{}},{"cell_type":"code","source":"hashmap = {}\nfor i , ch in enumerate(range(97,123)):\n    hashmap[chr(ch)] = i\nhashmap['.']=26\nhashmap","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.708167Z","iopub.execute_input":"2023-07-03T12:54:09.708507Z","iopub.status.idle":"2023-07-03T12:54:09.717042Z","shell.execute_reply.started":"2023-07-03T12:54:09.708476Z","shell.execute_reply":"2023-07-03T12:54:09.716078Z"},"trusted":true},"execution_count":452,"outputs":[{"execution_count":452,"output_type":"execute_result","data":{"text/plain":"{'a': 0,\n 'b': 1,\n 'c': 2,\n 'd': 3,\n 'e': 4,\n 'f': 5,\n 'g': 6,\n 'h': 7,\n 'i': 8,\n 'j': 9,\n 'k': 10,\n 'l': 11,\n 'm': 12,\n 'n': 13,\n 'o': 14,\n 'p': 15,\n 'q': 16,\n 'r': 17,\n 's': 18,\n 't': 19,\n 'u': 20,\n 'v': 21,\n 'w': 22,\n 'x': 23,\n 'y': 24,\n 'z': 25,\n '.': 26}"},"metadata":{}}]},{"cell_type":"code","source":"x_num = []\ny_num = []\nfor i in range(len(x)):\n    tmp = []\n    for ch in x[i]:\n        tmp.append(hashmap[ch])\n    y_num.append(hashmap[y[i]])\n    x_num.append(tmp)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:09.722451Z","iopub.execute_input":"2023-07-03T12:54:09.723146Z","iopub.status.idle":"2023-07-03T12:54:10.445706Z","shell.execute_reply.started":"2023-07-03T12:54:09.723111Z","shell.execute_reply":"2023-07-03T12:54:10.444671Z"},"trusted":true},"execution_count":453,"outputs":[]},{"cell_type":"code","source":"x_num[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.447253Z","iopub.execute_input":"2023-07-03T12:54:10.447602Z","iopub.status.idle":"2023-07-03T12:54:10.458312Z","shell.execute_reply.started":"2023-07-03T12:54:10.447566Z","shell.execute_reply":"2023-07-03T12:54:10.457104Z"},"trusted":true},"execution_count":454,"outputs":[{"execution_count":454,"output_type":"execute_result","data":{"text/plain":"[[26, 26, 26, 26],\n [26, 26, 26, 4],\n [26, 26, 4, 12],\n [26, 4, 12, 12],\n [26, 26, 26, 26]]"},"metadata":{}}]},{"cell_type":"code","source":"y_num[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.459979Z","iopub.execute_input":"2023-07-03T12:54:10.460335Z","iopub.status.idle":"2023-07-03T12:54:10.471487Z","shell.execute_reply.started":"2023-07-03T12:54:10.460311Z","shell.execute_reply":"2023-07-03T12:54:10.470492Z"},"trusted":true},"execution_count":455,"outputs":[{"execution_count":455,"output_type":"execute_result","data":{"text/plain":"[4, 12, 12, 0, 14]"},"metadata":{}}]},{"cell_type":"code","source":"x_num = torch.tensor(x_num)\ny_num = torch.tensor(y_num)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.473635Z","iopub.execute_input":"2023-07-03T12:54:10.474376Z","iopub.status.idle":"2023-07-03T12:54:10.816165Z","shell.execute_reply.started":"2023-07-03T12:54:10.474344Z","shell.execute_reply":"2023-07-03T12:54:10.815161Z"},"trusted":true},"execution_count":456,"outputs":[]},{"cell_type":"code","source":"x_num.shape\nprint(x_num[:10])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.817930Z","iopub.execute_input":"2023-07-03T12:54:10.818327Z","iopub.status.idle":"2023-07-03T12:54:10.825311Z","shell.execute_reply.started":"2023-07-03T12:54:10.818292Z","shell.execute_reply":"2023-07-03T12:54:10.824190Z"},"trusted":true},"execution_count":457,"outputs":[{"name":"stdout","text":"tensor([[26, 26, 26, 26],\n        [26, 26, 26,  4],\n        [26, 26,  4, 12],\n        [26,  4, 12, 12],\n        [26, 26, 26, 26],\n        [26, 26, 26, 14],\n        [26, 26, 14, 11],\n        [26, 14, 11,  8],\n        [14, 11,  8, 21],\n        [11,  8, 21,  8]])\n","output_type":"stream"}]},{"cell_type":"code","source":"y_num.shape\nprint(y_num[:10])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.827083Z","iopub.execute_input":"2023-07-03T12:54:10.827700Z","iopub.status.idle":"2023-07-03T12:54:10.835127Z","shell.execute_reply.started":"2023-07-03T12:54:10.827650Z","shell.execute_reply":"2023-07-03T12:54:10.834087Z"},"trusted":true},"execution_count":458,"outputs":[{"name":"stdout","text":"tensor([ 4, 12, 12,  0, 14, 11,  8, 21,  8,  0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**approach**\n1. *a trigram laungauge model is a model where it take input of 3 sequence characters and predicts the next char in the sequence*\n2. *this can be used as document filling model*\n3. *here i will be implementing a trigram model with wavenet architecture*  ","metadata":{}},{"cell_type":"markdown","source":"*input after will be (3,3) encoded vector out put will be a softmax of dim->(27,1) each giving probs of each char*\n*considering batch size input will be (batch_size , 3) -> (batch_size,3,3) -> (batch_size,9) -> *","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embed = nn.Embedding(27,4)\n        self.linear64 = nn.Linear(16,64)\n#         self.norm1= nn.BatchNorm1d(2)\n        self.linear256 = nn.Linear(64,256)\n#         self.norm2=nn.BatchNorm1d(1)\n        self.linear128 =nn.Linear(256,128)\n#         self.norm3 = nn.BatchNorm1d(1)\n        self.linear = nn.Linear(128,27)\n        self.logits = nn.Softmax(dim=1)\n    def forward(self, x):\n        # 64 , 4 ,4\n#         x = x.to('cpu')\n        x = self.embed(x)\n        x = x.view(64,16)\n        x = self.linear64(x)\n#         x = self.norm1(x)\n#         x = x.view(64,1,128)\n        x = self.linear256(x)\n#         x = self.norm2(x)\n        # 64 1 256\n        x = self.linear128(x)\n#         x = self.norm3(x)\n        x = self.linear(x)\n        x = self.logits(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-03T13:00:08.339005Z","iopub.execute_input":"2023-07-03T13:00:08.339714Z","iopub.status.idle":"2023-07-03T13:00:08.348554Z","shell.execute_reply.started":"2023-07-03T13:00:08.339681Z","shell.execute_reply":"2023-07-03T13:00:08.347495Z"},"trusted":true},"execution_count":478,"outputs":[]},{"cell_type":"code","source":"class textset(Dataset):\n    def __init__(self,x_num,y_num):\n        self.x=x_num\n        self.y=y_num\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self,idx):\n        x=self.x[idx]\n        y=self.y[idx]\n        return x.to(device) , y.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.848344Z","iopub.execute_input":"2023-07-03T12:54:10.848864Z","iopub.status.idle":"2023-07-03T12:54:10.857912Z","shell.execute_reply.started":"2023-07-03T12:54:10.848831Z","shell.execute_reply":"2023-07-03T12:54:10.856844Z"},"trusted":true},"execution_count":460,"outputs":[]},{"cell_type":"code","source":"data = textset(x_num,y_num)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.858920Z","iopub.execute_input":"2023-07-03T12:54:10.860309Z","iopub.status.idle":"2023-07-03T12:54:10.870421Z","shell.execute_reply.started":"2023-07-03T12:54:10.860275Z","shell.execute_reply":"2023-07-03T12:54:10.869406Z"},"trusted":true},"execution_count":461,"outputs":[]},{"cell_type":"code","source":"loader = DataLoader(data,batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.873482Z","iopub.execute_input":"2023-07-03T12:54:10.874173Z","iopub.status.idle":"2023-07-03T12:54:10.883020Z","shell.execute_reply.started":"2023-07-03T12:54:10.874138Z","shell.execute_reply":"2023-07-03T12:54:10.881899Z"},"trusted":true},"execution_count":462,"outputs":[]},{"cell_type":"code","source":"trigram = Model().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T13:00:17.815684Z","iopub.execute_input":"2023-07-03T13:00:17.816054Z","iopub.status.idle":"2023-07-03T13:00:17.824770Z","shell.execute_reply.started":"2023-07-03T13:00:17.816007Z","shell.execute_reply":"2023-07-03T13:00:17.823770Z"},"trusted":true},"execution_count":479,"outputs":[]},{"cell_type":"code","source":"summary(trigram,(1,4),64)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:57:27.998905Z","iopub.execute_input":"2023-07-03T12:57:27.999310Z","iopub.status.idle":"2023-07-03T12:57:28.107354Z","shell.execute_reply.started":"2023-07-03T12:57:27.999281Z","shell.execute_reply":"2023-07-03T12:57:28.105852Z"},"trusted":true},"execution_count":474,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[474], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrigram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[472], line 17\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# 64 , 4 ,4\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         x = x.to('cpu')\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(x\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear64(x)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         x = self.norm1(x)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         x = x.view(64,1,128)\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 16]' is invalid for input of size 32"],"ename":"RuntimeError","evalue":"shape '[64, 16]' is invalid for input of size 32","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(trigram.parameters(),lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:10.921116Z","iopub.execute_input":"2023-07-03T12:54:10.921437Z","iopub.status.idle":"2023-07-03T12:54:10.925987Z","shell.execute_reply.started":"2023-07-03T12:54:10.921412Z","shell.execute_reply":"2023-07-03T12:54:10.924946Z"},"trusted":true},"execution_count":465,"outputs":[]},{"cell_type":"code","source":"for parm in trigram.parameters():\n    print(parm[:5])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T13:00:27.667757Z","iopub.execute_input":"2023-07-03T13:00:27.668142Z","iopub.status.idle":"2023-07-03T13:00:27.786129Z","shell.execute_reply.started":"2023-07-03T13:00:27.668110Z","shell.execute_reply":"2023-07-03T13:00:27.784944Z"},"trusted":true},"execution_count":480,"outputs":[{"name":"stdout","text":"tensor([[ 1.1846,  1.1537,  0.3813,  2.2242],\n        [ 0.7513,  1.0656,  1.0289, -0.4807],\n        [-0.5596,  0.8705,  0.4391,  0.0615],\n        [-1.0894, -0.1862, -0.1217,  1.2825],\n        [ 1.7309,  1.1174, -0.0812,  0.0745]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[-2.1493e-01,  2.8640e-05, -1.7397e-02,  7.0398e-02, -1.1500e-01,\n          1.1547e-01,  2.4832e-01, -3.9346e-02, -3.3767e-02, -3.5864e-02,\n          1.4336e-01,  2.9966e-02,  9.1847e-02,  8.9076e-03,  4.3572e-02,\n         -4.0713e-02],\n        [ 2.3830e-01, -6.9435e-02,  1.1514e-01,  2.1991e-01,  1.4444e-01,\n          2.2016e-02, -4.8118e-02,  2.7755e-02,  1.4928e-01,  6.3503e-02,\n          2.1662e-02,  2.3529e-01, -6.8684e-02, -6.0639e-02,  5.7035e-02,\n          1.2135e-01],\n        [ 5.7495e-02,  2.3843e-01, -5.2311e-02,  1.2087e-01, -9.4072e-02,\n         -1.7996e-01,  1.1324e-01,  1.8237e-01,  1.3722e-01,  1.4455e-01,\n          4.7336e-02, -2.1342e-01, -1.3485e-01, -3.9537e-02,  6.5599e-02,\n         -6.5014e-02],\n        [ 2.2117e-01, -1.6820e-01,  1.7615e-01,  9.2555e-02, -1.0885e-01,\n         -9.1701e-03, -2.3381e-01, -4.9702e-02,  1.7551e-01,  2.6058e-02,\n         -1.3736e-01,  2.3977e-01, -4.8801e-02, -2.1441e-01, -3.0995e-03,\n          6.1981e-02],\n        [-1.6327e-02,  2.2769e-01,  1.8719e-01, -1.4156e-02,  1.0018e-01,\n         -2.2350e-01, -9.6293e-02, -2.4388e-01,  9.7464e-02,  2.4072e-01,\n         -3.7452e-02, -1.8769e-01,  1.2470e-01, -6.8913e-03, -6.4493e-02,\n         -3.4947e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\ntensor([-0.2226,  0.0535,  0.1940, -0.0543,  0.2141], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[-5.5587e-02, -5.7184e-02,  7.9551e-03, -6.2773e-02, -1.0792e-01,\n          7.3532e-02, -1.1153e-01,  3.0074e-02, -1.2326e-01, -4.3481e-02,\n          1.2418e-01,  8.0563e-03, -6.9499e-02,  9.5628e-02,  5.5468e-02,\n          1.6238e-02, -7.5948e-03, -1.1888e-01,  3.2935e-02, -6.0752e-05,\n         -3.2086e-02,  6.3260e-02,  3.6831e-02,  1.1297e-01,  7.5947e-02,\n          3.8466e-02,  8.8747e-02,  7.7637e-02, -6.7935e-02,  8.2967e-04,\n         -2.8537e-02, -1.1646e-01, -7.3914e-02, -2.0215e-02, -7.1487e-02,\n         -9.0171e-02,  1.2086e-01,  9.2102e-02, -8.8251e-02, -1.0275e-01,\n         -9.8057e-02, -7.1725e-02,  5.1395e-02, -3.2714e-02,  9.3562e-02,\n          1.0490e-01,  2.6246e-02,  6.3434e-02,  3.8474e-02, -8.7199e-04,\n         -1.0629e-01,  3.9546e-02,  6.9425e-02,  2.0323e-02, -1.5023e-02,\n          6.5807e-02,  1.1075e-01,  5.2667e-02, -8.9941e-02,  1.2284e-01,\n         -9.1470e-02,  1.0427e-01, -1.0852e-01,  7.3577e-03],\n        [ 8.2409e-02,  9.2487e-02,  3.3570e-02, -1.0977e-01, -1.3985e-02,\n         -2.8351e-02,  7.6824e-02, -4.1190e-02, -4.4856e-02, -4.4141e-02,\n         -9.1418e-02, -1.1144e-03, -6.4109e-02, -8.5299e-02, -1.2405e-01,\n          1.4565e-02,  1.2420e-02,  1.6781e-02, -9.8749e-02,  1.1908e-02,\n          1.0294e-01,  1.1949e-01,  2.5454e-02, -1.1789e-01, -9.9449e-02,\n          1.6161e-02, -7.1715e-02, -3.3285e-02, -2.1393e-02, -9.0824e-02,\n         -8.5664e-02,  9.5300e-02, -2.8955e-02, -1.4879e-02, -9.8438e-02,\n          3.5563e-02,  9.8618e-02, -8.3214e-02,  2.2940e-02,  3.3016e-02,\n          4.9710e-02, -4.3156e-02,  6.9011e-02,  7.1746e-02,  5.7608e-02,\n         -7.8011e-02,  9.8045e-02, -5.9410e-02, -7.1404e-02, -6.9778e-02,\n          3.0548e-03, -9.3078e-02,  2.3485e-02,  9.5578e-03,  4.0791e-02,\n         -1.0996e-01,  1.1911e-01,  3.6426e-02, -2.6898e-02,  1.1300e-01,\n          8.2006e-02, -1.0386e-01, -5.7124e-02, -3.4257e-02],\n        [ 3.6424e-02,  2.4890e-02, -1.0405e-01,  6.4587e-02,  7.4170e-02,\n         -5.2133e-02, -6.7607e-02,  6.2184e-02,  4.6422e-02,  1.0025e-01,\n         -2.9858e-02,  2.1894e-02,  6.0634e-02, -3.2430e-02, -4.2624e-02,\n         -9.5953e-02, -2.7395e-02,  1.1230e-01, -9.7563e-02, -2.3958e-02,\n         -6.4901e-02, -8.3676e-02, -7.4133e-02,  8.9832e-02, -1.1619e-01,\n         -4.6117e-02,  3.5869e-04,  1.0872e-01,  1.1288e-02,  3.0546e-02,\n          6.0112e-02,  2.9053e-02, -2.6243e-02, -1.1525e-01, -8.4371e-02,\n         -1.0096e-01, -1.1926e-01,  1.0759e-01,  1.1471e-01, -1.5701e-02,\n          1.8621e-02, -8.7522e-02,  7.8264e-02, -3.6327e-02,  1.0325e-01,\n         -5.9647e-02, -9.3437e-02, -4.2144e-02,  1.0431e-01,  1.5422e-02,\n         -6.0617e-02, -2.3670e-02,  7.7265e-02,  4.8262e-02, -2.5206e-02,\n          2.4656e-02, -9.4429e-02, -8.9777e-02, -7.9778e-02,  1.2622e-02,\n         -9.1496e-02,  1.1722e-01, -9.5517e-02, -6.4783e-02],\n        [ 1.1024e-01, -1.2098e-01,  1.1925e-01, -2.4257e-02, -7.7417e-04,\n          5.8256e-02, -9.3930e-02, -3.5077e-02,  7.4137e-02,  6.9765e-02,\n         -7.9310e-02,  9.3632e-02, -1.0094e-01, -1.0011e-02, -6.9999e-02,\n          7.6318e-02,  7.3061e-02,  9.5461e-02, -1.0973e-02,  1.1482e-01,\n          4.1747e-02, -1.0871e-01, -3.2843e-02,  4.5531e-02,  3.9327e-02,\n         -2.0573e-03, -3.9576e-02,  4.1497e-02,  7.7314e-02,  1.0024e-01,\n          2.2171e-02,  9.3956e-02, -7.9969e-02,  1.1160e-01, -7.4068e-02,\n         -2.5449e-02,  8.3899e-02,  6.6641e-02,  5.6363e-02,  9.7061e-02,\n          4.6651e-02, -7.3074e-02,  8.5726e-02,  7.0143e-02, -1.7833e-02,\n         -2.1143e-02, -1.2077e-01, -5.2918e-02,  5.8650e-02,  8.2478e-02,\n         -3.8646e-02, -1.1158e-01,  5.0797e-03,  2.7943e-02,  5.1404e-02,\n          1.1204e-01, -2.8244e-02, -7.6701e-02,  1.1333e-01,  6.4426e-02,\n         -1.1136e-01,  8.1666e-02,  8.5988e-02, -1.6898e-02],\n        [-3.0435e-02,  6.4569e-02, -1.0901e-01,  5.8418e-02,  2.2938e-02,\n          9.2021e-02,  1.8111e-02, -6.0111e-02, -9.1631e-02, -1.0390e-01,\n         -1.2020e-01, -2.6620e-02, -8.1226e-02,  7.2239e-02,  4.5064e-02,\n         -6.4833e-02,  6.5692e-03, -1.0759e-01, -1.0818e-01,  8.3267e-02,\n         -9.2777e-02,  3.3534e-02, -6.0254e-02, -7.0413e-02,  5.6813e-02,\n         -5.8528e-02,  1.0210e-01,  1.0290e-02, -9.0222e-02, -8.3194e-02,\n         -5.3880e-03, -8.0047e-02,  1.1034e-01,  5.0245e-02, -7.7286e-02,\n         -3.6896e-02,  6.7921e-02, -1.5266e-02,  1.0594e-01,  9.7780e-02,\n         -4.6440e-02,  2.2935e-02,  1.0725e-01,  7.2936e-02, -1.6350e-02,\n         -3.2923e-02,  1.2210e-01,  8.8745e-02, -8.9885e-02, -6.0966e-03,\n          1.1170e-01,  1.1437e-01,  4.8658e-02,  7.9916e-02, -9.8096e-02,\n         -7.1221e-02,  9.2968e-02, -7.0872e-02, -4.8630e-02,  2.7996e-02,\n          5.1984e-02, -1.2132e-02, -9.0155e-02,  6.3419e-05]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([-0.0298,  0.0069,  0.0778,  0.1103,  0.0336], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[ 0.0319, -0.0418, -0.0227,  ...,  0.0152,  0.0425,  0.0039],\n        [-0.0526, -0.0053,  0.0062,  ..., -0.0127,  0.0105, -0.0515],\n        [ 0.0299,  0.0136, -0.0311,  ..., -0.0486,  0.0094, -0.0272],\n        [ 0.0263, -0.0373,  0.0149,  ..., -0.0468, -0.0582, -0.0464],\n        [ 0.0177, -0.0222,  0.0510,  ...,  0.0262, -0.0054, -0.0024]],\n       device='cuda:0', grad_fn=<SliceBackward0>)\ntensor([ 0.0327,  0.0119,  0.0038, -0.0380, -0.0264], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([[-0.0725, -0.0341,  0.0399, -0.0115, -0.0307, -0.0281, -0.0373, -0.0183,\n         -0.0720,  0.0025,  0.0400,  0.0647,  0.0882,  0.0836, -0.0578, -0.0878,\n         -0.0567, -0.0689, -0.0255,  0.0710,  0.0238,  0.0878,  0.0591, -0.0234,\n         -0.0451, -0.0433, -0.0646,  0.0281,  0.0665,  0.0784, -0.0594, -0.0473,\n         -0.0253,  0.0622,  0.0745,  0.0798,  0.0316,  0.0146, -0.0338,  0.0745,\n         -0.0410,  0.0213, -0.0179, -0.0397,  0.0106,  0.0337,  0.0626, -0.0411,\n          0.0428,  0.0717,  0.0092,  0.0808, -0.0513,  0.0339,  0.0386,  0.0309,\n          0.0492,  0.0600,  0.0215,  0.0854, -0.0498, -0.0567, -0.0567, -0.0864,\n          0.0156, -0.0154, -0.0869, -0.0829,  0.0266,  0.0046,  0.0082, -0.0324,\n         -0.0234,  0.0022,  0.0064, -0.0109,  0.0104,  0.0621, -0.0328, -0.0608,\n          0.0155, -0.0857,  0.0537, -0.0872,  0.0125,  0.0024, -0.0397, -0.0251,\n          0.0312, -0.0873,  0.0785,  0.0509,  0.0408, -0.0139,  0.0425, -0.0197,\n          0.0064, -0.0091, -0.0682,  0.0654,  0.0466, -0.0269, -0.0573, -0.0299,\n         -0.0627, -0.0611, -0.0086,  0.0311, -0.0041,  0.0810,  0.0212,  0.0183,\n          0.0742, -0.0456,  0.0350, -0.0726, -0.0822,  0.0320, -0.0469, -0.0304,\n          0.0319,  0.0751, -0.0649, -0.0003, -0.0365, -0.0050,  0.0171,  0.0343],\n        [ 0.0128, -0.0484,  0.0812,  0.0688,  0.0298, -0.0849,  0.0103, -0.0433,\n         -0.0604,  0.0628, -0.0099,  0.0144,  0.0054, -0.0177, -0.0213, -0.0022,\n         -0.0400, -0.0870,  0.0485,  0.0467, -0.0385,  0.0543, -0.0629,  0.0878,\n          0.0097, -0.0457, -0.0383,  0.0296,  0.0612,  0.0356, -0.0724, -0.0457,\n          0.0844,  0.0602,  0.0263, -0.0532,  0.0574,  0.0791,  0.0792,  0.0660,\n          0.0500,  0.0229,  0.0771,  0.0025, -0.0428, -0.0656,  0.0380,  0.0239,\n          0.0447,  0.0264, -0.0800,  0.0586,  0.0004, -0.0609, -0.0774, -0.0823,\n         -0.0353,  0.0563,  0.0267,  0.0604,  0.0636, -0.0876, -0.0214, -0.0063,\n          0.0503, -0.0242,  0.0555, -0.0715, -0.0736,  0.0587, -0.0612, -0.0196,\n          0.0826,  0.0310, -0.0864,  0.0204,  0.0106, -0.0230,  0.0455,  0.0856,\n         -0.0476,  0.0827, -0.0550,  0.0348,  0.0348,  0.0478, -0.0789,  0.0817,\n         -0.0446,  0.0061, -0.0372, -0.0703, -0.0466,  0.0315,  0.0575, -0.0111,\n         -0.0627, -0.0651, -0.0216, -0.0691, -0.0755,  0.0409,  0.0139,  0.0819,\n          0.0562, -0.0361, -0.0341,  0.0258, -0.0751, -0.0829, -0.0728, -0.0167,\n          0.0548,  0.0657,  0.0706,  0.0435,  0.0386,  0.0631, -0.0497, -0.0751,\n          0.0401,  0.0233, -0.0559,  0.0311, -0.0754,  0.0549,  0.0861,  0.0107],\n        [ 0.0729, -0.0173,  0.0496,  0.0050,  0.0844, -0.0574, -0.0159,  0.0302,\n          0.0411, -0.0002, -0.0640, -0.0268,  0.0881,  0.0199, -0.0726, -0.0128,\n         -0.0883,  0.0775,  0.0794, -0.0432,  0.0304,  0.0861, -0.0380,  0.0771,\n         -0.0231,  0.0221,  0.0429, -0.0054,  0.0032, -0.0227,  0.0093,  0.0478,\n         -0.0154, -0.0800, -0.0630, -0.0744, -0.0065,  0.0789,  0.0391, -0.0048,\n         -0.0321,  0.0492,  0.0877,  0.0234,  0.0114, -0.0027, -0.0301, -0.0081,\n          0.0795, -0.0157, -0.0696, -0.0101,  0.0223, -0.0355,  0.0685,  0.0618,\n          0.0844,  0.0297,  0.0525,  0.0550, -0.0581, -0.0467,  0.0847, -0.0731,\n         -0.0018, -0.0475, -0.0396, -0.0631,  0.0046, -0.0597,  0.0821,  0.0683,\n         -0.0534,  0.0619,  0.0624, -0.0746,  0.0418, -0.0704, -0.0503, -0.0705,\n         -0.0745,  0.0059,  0.0842,  0.0818,  0.0475,  0.0216,  0.0620,  0.0853,\n         -0.0474,  0.0274, -0.0097, -0.0275,  0.0116,  0.0708,  0.0058, -0.0164,\n          0.0174,  0.0373, -0.0729, -0.0584, -0.0616, -0.0287,  0.0187,  0.0254,\n          0.0668, -0.0450, -0.0202, -0.0483,  0.0656, -0.0733,  0.0413, -0.0851,\n          0.0077,  0.0684,  0.0099,  0.0872,  0.0882,  0.0290, -0.0140, -0.0350,\n          0.0140,  0.0449,  0.0492,  0.0267,  0.0573,  0.0609,  0.0352, -0.0580],\n        [-0.0366,  0.0074,  0.0534, -0.0057,  0.0021,  0.0041, -0.0099,  0.0599,\n          0.0486, -0.0419, -0.0622,  0.0286, -0.0443, -0.0632,  0.0673,  0.0474,\n         -0.0436,  0.0716, -0.0617, -0.0081, -0.0510,  0.0339, -0.0129, -0.0185,\n         -0.0760,  0.0475, -0.0055,  0.0518, -0.0568,  0.0852,  0.0682,  0.0078,\n          0.0872,  0.0769,  0.0785, -0.0812, -0.0028,  0.0390,  0.0349, -0.0874,\n         -0.0880,  0.0267,  0.0482, -0.0065, -0.0553,  0.0810, -0.0416, -0.0593,\n          0.0768, -0.0029,  0.0120, -0.0561, -0.0191, -0.0613, -0.0795, -0.0615,\n         -0.0268,  0.0171,  0.0822, -0.0037,  0.0079,  0.0373, -0.0870,  0.0792,\n          0.0442, -0.0737,  0.0371, -0.0292, -0.0067,  0.0232,  0.0300, -0.0120,\n          0.0756,  0.0301,  0.0725, -0.0132, -0.0018,  0.0617, -0.0873,  0.0327,\n         -0.0604,  0.0483,  0.0007, -0.0075,  0.0650,  0.0391, -0.0460,  0.0620,\n         -0.0397,  0.0542, -0.0375, -0.0629,  0.0599, -0.0445, -0.0461,  0.0082,\n         -0.0377, -0.0383,  0.0021, -0.0256, -0.0727, -0.0265, -0.0617,  0.0796,\n         -0.0798,  0.0577, -0.0285, -0.0618,  0.0688,  0.0142,  0.0010,  0.0289,\n         -0.0207,  0.0556, -0.0152,  0.0634, -0.0280, -0.0421,  0.0166, -0.0010,\n         -0.0822,  0.0016,  0.0429,  0.0806,  0.0653, -0.0075, -0.0368,  0.0239],\n        [ 0.0443, -0.0296, -0.0541, -0.0201, -0.0791,  0.0783,  0.0005,  0.0548,\n          0.0416,  0.0657,  0.0051,  0.0164,  0.0016, -0.0539, -0.0351,  0.0039,\n         -0.0529,  0.0055, -0.0023,  0.0502,  0.0529, -0.0670, -0.0158, -0.0809,\n         -0.0514,  0.0011,  0.0481, -0.0733,  0.0349,  0.0754,  0.0121,  0.0282,\n         -0.0621,  0.0829, -0.0188, -0.0408, -0.0842, -0.0029,  0.0641,  0.0084,\n         -0.0387, -0.0704,  0.0328, -0.0021, -0.0112, -0.0436,  0.0776, -0.0191,\n          0.0106,  0.0212,  0.0401,  0.0397, -0.0334,  0.0783, -0.0690, -0.0756,\n         -0.0412,  0.0290, -0.0089,  0.0555, -0.0040, -0.0323,  0.0358, -0.0754,\n          0.0604,  0.0180,  0.0681,  0.0376, -0.0436, -0.0793, -0.0790,  0.0233,\n         -0.0354,  0.0476,  0.0168, -0.0064, -0.0317, -0.0331, -0.0539,  0.0347,\n         -0.0791,  0.0387,  0.0836, -0.0130,  0.0389, -0.0155, -0.0388, -0.0267,\n         -0.0812, -0.0805, -0.0658,  0.0638,  0.0865,  0.0248,  0.0036,  0.0729,\n         -0.0655,  0.0133,  0.0317, -0.0107,  0.0025, -0.0617, -0.0750,  0.0225,\n         -0.0324, -0.0585, -0.0751, -0.0824,  0.0484,  0.0586, -0.0423,  0.0106,\n          0.0347,  0.0410,  0.0595, -0.0784, -0.0882, -0.0522,  0.0026,  0.0586,\n          0.0301, -0.0621,  0.0679, -0.0837,  0.0506,  0.0817,  0.0393, -0.0489]],\n       device='cuda:0', grad_fn=<SliceBackward0>)\ntensor([ 0.0330,  0.0268,  0.0606, -0.0042, -0.0713], device='cuda:0',\n       grad_fn=<SliceBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"trigram.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T13:01:55.367848Z","iopub.execute_input":"2023-07-03T13:01:55.368296Z","iopub.status.idle":"2023-07-03T13:01:55.375887Z","shell.execute_reply.started":"2023-07-03T13:01:55.368260Z","shell.execute_reply":"2023-07-03T13:01:55.374764Z"},"trusted":true},"execution_count":482,"outputs":[{"execution_count":482,"output_type":"execute_result","data":{"text/plain":"Model(\n  (embed): Embedding(27, 4)\n  (linear64): Linear(in_features=16, out_features=64, bias=True)\n  (linear256): Linear(in_features=64, out_features=256, bias=True)\n  (linear128): Linear(in_features=256, out_features=128, bias=True)\n  (linear): Linear(in_features=128, out_features=27, bias=True)\n  (logits): Softmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 2000\nlosses = []\nfor i in range(epochs):\n    running_loss = 0\n    for x , y in tqdm(loader):\n        if(x.shape[0]!=64):\n            print(f\"loss at {i}th epoch is : {running_loss}\")\n            continue\n        pred = trigram(x)\n        print(pred)\n        loss = F.cross_entropy(pred,y)\n        loss.backward()\n        print(loss.grad)\n        break\n        running_loss+=loss.item()\n#         tqdm.set_description(f\"current batch running loss: {running_loss}\")\n    break\n    print(f\"loss at {i}th epoch is : {running_loss}\")\n    losses.append(running_loss)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T13:09:14.619007Z","iopub.execute_input":"2023-07-03T13:09:14.620101Z","iopub.status.idle":"2023-07-03T13:09:14.642900Z","shell.execute_reply.started":"2023-07-03T13:09:14.620037Z","shell.execute_reply":"2023-07-03T13:09:14.641766Z"},"trusted":true},"execution_count":494,"outputs":[{"name":"stderr","text":"  0%|          | 0/2456 [00:00<?, ?it/s]/tmp/ipykernel_28/1249928908.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /usr/local/src/pytorch/build/aten/src/ATen/core/TensorBody.h:486.)\n  print(loss.grad)\n  0%|          | 0/2456 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([[0.0431, 0.0408, 0.0395,  ..., 0.0406, 0.0402, 0.0306],\n        [0.0414, 0.0373, 0.0392,  ..., 0.0420, 0.0410, 0.0344],\n        [0.0437, 0.0404, 0.0349,  ..., 0.0394, 0.0406, 0.0333],\n        ...,\n        [0.0368, 0.0334, 0.0383,  ..., 0.0351, 0.0381, 0.0429],\n        [0.0341, 0.0321, 0.0328,  ..., 0.0404, 0.0390, 0.0406],\n        [0.0411, 0.0359, 0.0314,  ..., 0.0359, 0.0403, 0.0341]],\n       device='cuda:0', grad_fn=<SoftmaxBackward0>)\nNone\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# plt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T12:54:11.006944Z","iopub.status.idle":"2023-07-03T12:54:11.007964Z","shell.execute_reply.started":"2023-07-03T12:54:11.007716Z","shell.execute_reply":"2023-07-03T12:54:11.007740Z"},"trusted":true},"execution_count":null,"outputs":[]}]}